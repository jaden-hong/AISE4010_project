{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTC Delay Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'AISE4010 (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n AISE4010 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from config import data_path\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "##import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'AISE4010 (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n AISE4010 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Defining parsing function\n",
    "\n",
    "def standardize_time(time_str):\n",
    "    # Add seconds if missing\n",
    "    if len(time_str.split(':')) == 2:  # Format is h:m\n",
    "        time_str += ':00'\n",
    "    return time_str\n",
    "\n",
    "def standardize_time_format(time_str):\n",
    "    try:\n",
    "        # Parse the time string\n",
    "        parsed_time = parse(str(time_str)).time()  # Extract only the time\n",
    "        # Format to HH:MM:SS\n",
    "        return parsed_time.strftime('%H:%M:%S')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse '{time_str}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def standardize_date_format(date_str):\n",
    "    try:\n",
    "        # Parse the date string\n",
    "        parsed_date = parse(str(date_str)) \n",
    "        # Format to YYYY-MM-DD\n",
    "        return parsed_date.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse '{date_str}': {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadRawData(type=\"bus\",start_year = 2014, end_year = 2015,targets = [], features = [],file_path = 'a'):\n",
    "    \"\"\"\n",
    "    type = bus, subway, streetcar\n",
    "    start_year = start of year range\n",
    "    end_year = end of year range\n",
    "    targets = targets of dataset\n",
    "    features = features of dataset\n",
    "\n",
    "    \n",
    "    loads data, based off given parameters\n",
    "    \"\"\"\n",
    "    subfolder_path = os.path.join(data_path, type)\n",
    "\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "            raise ValueError(f\"Subfolder '{type}' does not exist in {data_path}.\") #making sure path is correct\n",
    "    \n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(subfolder_path):\n",
    "        # print(\"On filename:\",filename)\n",
    "        if (\n",
    "            filename.endswith(\".xlsx\") and\n",
    "            filename.startswith(f\"ttc-{type}-delay-data\") and\n",
    "            start_year <= int(filename.split(\"-\")[-1].split(\".\")[0]) <= end_year\n",
    "        ):\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "                sheet_names = pd.ExcelFile(file_path).sheet_names\n",
    "                for month in sheet_names:\n",
    "                    data = pd.read_excel(file_path,sheet_name=month)\n",
    "\n",
    "                    # accounting for inconsistent data formatting\n",
    "                    if 'Report Date' in data.columns:\n",
    "                        data.rename(columns={'Report Date': 'Date'}, inplace=True)\n",
    "                    elif 'Date' in data.columns:\n",
    "                        pass  # Column is already named \"date\"\n",
    "\n",
    "                    if 'Delay' in data.columns:\n",
    "                        data.rename(columns={'Delay': 'Min Delay'}, inplace=True)\n",
    "                    elif 'Min Delay' in data.columns:\n",
    "                        pass  # Column is already named \"date\"\n",
    "                    \n",
    "                    all_data = pd.concat([all_data, data], ignore_index=True)    \n",
    "\n",
    "    # all_data.info()\n",
    "    # print(all_data.describe())\n",
    "    # print(\"\\n\")\n",
    "    return all_data\n",
    "\n",
    "def process_data(df,targets,features,start=\"-01-01\"):\n",
    "    '''\n",
    "    Takes in dataframe and preprocesses based off arguments \n",
    "    '''\n",
    "    # targets = \"min_delay\"\n",
    "    # features = [\"\",\"\",\"\"]\n",
    "    # print(\"Using features:\\n\",features,\"\\nTargets:\",targets)\n",
    "    df = df.sort_index()\n",
    "    df = df[targets+features] #only using necessary data\n",
    "    #drop empty rows:\n",
    "    df.dropna(axis=0, how='all', inplace=True) #drops where all are null\n",
    "    # print(df['Time'].head())\n",
    "    \n",
    "    df['Time'] = df['Time'].apply(standardize_time_format)\n",
    "\n",
    "    # combines the time with the date\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.strftime('%H:%M:%S')\n",
    "    df['Datetime'] = pd.to_datetime(df['Date'].dt.strftime('%Y-%m-%d') + ' ' + df['Time']) #combining into one column\n",
    "\n",
    "    #dropping the unecessary columns:\n",
    "    df.drop(columns = ['Time','Date'], inplace = True)\n",
    "    # print(df.columns)\n",
    "\n",
    "    # preprocessing the direction to make consistent 4 + 1 directions \n",
    "    valid_directions = ['n','s','e','w','b'] #should only have n,e,s,w, b - both ways\n",
    "\n",
    "    df['Direction'] = df['Direction'].str[0].str.lower()\n",
    "    df['Direction'] = df['Direction'].apply(lambda x: x if x in valid_directions else 'unknown')\n",
    "    \n",
    "    unique_directions = df['Direction'].unique() \n",
    "    # print(unique_directions)\n",
    "\n",
    "    #one hot encoding\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns # only categorical features selected\n",
    "    # categorical_features = ['Route','Direction']\n",
    "    for features in categorical_features:\n",
    "        df[features] = df[features].astype(str)\n",
    "    # df['Route'] = df['Route'].astype(str)\n",
    "    # df['Direction'] = df['Direction'].astype(str)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    encoded_features = encoder.fit_transform(df[categorical_features])\n",
    "    \n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    df.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "    df.set_index('Datetime',inplace=True)\n",
    "\n",
    "    # Extract year, month, day, hour, and minute from the Datetime index\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['hour'] = df.index.hour\n",
    "    df['minute'] = df.index.minute\n",
    "     \n",
    "    scaler = StandardScaler() #standard because we expect standard deviation\n",
    "    # scaler = MinMaxScaler() #min max because ...\n",
    "\n",
    "    df[['Min Delay']] = scaler.fit_transform(df[['Min Delay']])\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    \n",
    "    return df,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# libraries used for \n",
    "\n",
    "\n",
    "def getXandY(type=\"bus\",start_year=2014,end_year=2015):\n",
    "\n",
    "    df = loadRawData(type,start_year,end_year)\n",
    "\n",
    "    targets = [\"Min Delay\"]\n",
    "    features = [\"Date\", \"Time\",\"Direction\"]\n",
    "\n",
    "\n",
    "    df,scaler = process_data(df,targets=targets,features=features)\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    n_steps = 10\n",
    "    n_outputs = 1\n",
    "    \n",
    "    \n",
    "    X, y= create_sliding_windows(df,n_steps,n_outputs,target_column=targets[0])\n",
    "    # print(\"X shape:\",X.shape)\n",
    "    # print(\"y shape:\",y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(np.isnan(X_train).any(), np.isinf(X_train).any())\n",
    "    print(np.isnan(y_train).any(), np.isinf(y_train).any())\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "def create_sliding_windows(df, n_steps, n_outputs, target_column):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame into overlapping sliding windows.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame with features and target variable.\n",
    "    - n_steps: Number of time steps in the input sequence.\n",
    "    - n_outputs: Number of time steps in the output sequence.\n",
    "    - target_column: Name or index of the target column.\n",
    "\n",
    "    Returns:\n",
    "    - X: Numpy array of shape (num_samples, n_steps, num_features)\n",
    "    - y: Numpy array of shape (num_samples, n_outputs)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    if isinstance(target_column, str):\n",
    "        target_index = df.columns.get_loc(target_column)  # Get column index\n",
    "    else:\n",
    "        target_index = target_column\n",
    "\n",
    "    data = df.to_numpy()  # Convert to NumPy for efficiency\n",
    "    for i in range(len(data) - n_steps - n_outputs + 1):\n",
    "        # Include all columns except the target in X\n",
    "        X.append(data[i:i + n_steps, :])\n",
    "        # Use only the target column for y\n",
    "        y.append(data[i + n_steps:i + n_steps + n_outputs, target_index])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Exclude the target column from X (optional if the target is among features)\n",
    "    # X = np.delete(X, target_index, axis=-1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "## Creating models\n",
    "\n",
    "#parameters:\n",
    "type = 'bus'\n",
    "start_year = 2014 #min\n",
    "end_year = 2023 #max\n",
    "\n",
    "adam_lr = 0.00001\n",
    "num_neurons = 128\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "\n",
    "X_train, X_test, y_train, y_test, scaler = getXandY(type,start_year,end_year)\n",
    "input_shape = (X_train.shape[0],X_train.shape[2]) #num samples, num features in each sample\n",
    "\n",
    "lstm_model = models.Sequential()\n",
    "lstm_model.add(layers.LSTM(num_neurons,\n",
    "                           activation='relu',\n",
    "                           input_shape=input_shape))\n",
    "lstm_model.add(layers.Dense(1))  # for the final output layer since its only 1 output\n",
    "\n",
    "optimizer = Adam(learning_rate=adam_lr) #setting \n",
    "\n",
    "lstm_model.compile(optimizer=optimizer, metrics = ['mae'], loss='mse')  #use mae and mse since regression\n",
    "\n",
    "# creating model\n",
    "lstm_model.summary()\n",
    "\n",
    "#fitting model\n",
    "history = lstm_model.fit(X_train,y_train,\n",
    "                    epochs = num_epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_split=0.2, #0.2 of the training set to be used for validation\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing model:\n",
    "def test_model(history,model,X_train,X_test,y_train,y_test):\n",
    "\n",
    "    # getting predictions of model:\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # n_elements = len(y_pred_test)\n",
    "    # pad_size = 10 - (n_elements % 10)  # Calculate how much to pad\n",
    "    # y_pred_test_padded = np.pad(y_pred_test, (0, pad_size), mode='constant', constant_values=0)\n",
    "    # y_pred_test = y_pred_test_padded.reshape(-1, 10, 1)\n",
    "\n",
    "    # y_pred_train = lstm_model.predict(X_train)\n",
    "    # n_elements = len(y_pred_train)\n",
    "    # pad_size = 10 - (n_elements % 10)  # Calculate how much to pad\n",
    "    # y_pred_train_padded = np.pad(y_pred_train, (0, pad_size), mode='constant', constant_values=0)\n",
    "    # y_pred_train = y_pred_train_padded.reshape(-1, 10, 1) #since its not divisble by 10 ?\n",
    "\n",
    "    print(y_test.shape,y_test_pred.shape)\n",
    "    print(y_train.shape,y_train_pred.shape)\n",
    "\n",
    "    test_loss = model.evaluate(y_test, y_test_pred)\n",
    "    train_loss = model.evaluate(y_train,y_train_pred)\n",
    "\n",
    "    # # print(\"Predictions:\", y_pred)\n",
    "\n",
    "    print(f\"MSE training: {train_loss[0]}\")\n",
    "    print(f\"MSE testing: {test_loss[0]}\")\n",
    "\n",
    "    print(f'MAE train: {train_loss[1]}')\n",
    "    print(f'MAE test: {test_loss[1]}')\n",
    "\n",
    "\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "test_model(history,lstm_model,X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISE4010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
